<!doctype html><html lang=en><head><title>Comparing a transformer-based detector with a classical detector Â· Emanuele Viglianisi</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Emanuele Viglianisi"><meta name=description content="Table Of Contents ğŸ”¥ Motivation ğŸ”« Dataset Description ğŸ”© Set-Up ğŸ©ºResults ğŸ‘ Coco Metric CNN ğŸ‘ Coco Metric Transformers ğŸ™ Comments & Feedback Follow me ğŸ‘‡ ğŸ”¥ Motivation Link to heading Since the introduction, by Facebook, Transformer based detectors are getting a lot of traction in Computer Vision and Object Detection.
In this blog, I will show how a partially trained detector using SWIN transformer backbone leads to a very competitive performance compared to a fully trained classical detector."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Comparing a transformer-based detector with a classical detector"><meta name=twitter:description content="Table Of Contents ğŸ”¥ Motivation ğŸ”« Dataset Description ğŸ”© Set-Up ğŸ©ºResults ğŸ‘ Coco Metric CNN ğŸ‘ Coco Metric Transformers ğŸ™ Comments & Feedback Follow me ğŸ‘‡ ğŸ”¥ Motivation Link to heading Since the introduction, by Facebook, Transformer based detectors are getting a lot of traction in Computer Vision and Object Detection.
In this blog, I will show how a partially trained detector using SWIN transformer backbone leads to a very competitive performance compared to a fully trained classical detector."><meta property="og:title" content="Comparing a transformer-based detector with a classical detector"><meta property="og:description" content="Table Of Contents ğŸ”¥ Motivation ğŸ”« Dataset Description ğŸ”© Set-Up ğŸ©ºResults ğŸ‘ Coco Metric CNN ğŸ‘ Coco Metric Transformers ğŸ™ Comments & Feedback Follow me ğŸ‘‡ ğŸ”¥ Motivation Link to heading Since the introduction, by Facebook, Transformer based detectors are getting a lot of traction in Computer Vision and Object Detection.
In this blog, I will show how a partially trained detector using SWIN transformer backbone leads to a very competitive performance compared to a fully trained classical detector."><meta property="og:type" content="article"><meta property="og:url" content="https://emavgl.github.io/posts/third-blog/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-02T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-02T00:00:00+00:00"><link rel=canonical href=https://emavgl.github.io/posts/third-blog/><link rel=preload href="https://emavgl.github.io/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://emavgl.github.io/css/coder.min.ea4c355c5f9913809f506132a80bf3fab84f2679dee370f334f7385a36d24c38.css integrity="sha256-6kw1XF+ZE4CfUGEyqAvz+rhPJnne43DzNPc4WjbSTDg=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=https://emavgl.github.io/images/favicon.svg sizes=any><link rel=icon type=image/png href=https://emavgl.github.io/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://emavgl.github.io/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://emavgl.github.io/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=https://emavgl.github.io/images/apple-touch-icon.png><link rel=manifest href=https://emavgl.github.io/site.webmanifest><link rel=mask-icon href=https://emavgl.github.io/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><head></head><nav class=navigation><section class=container><a class=navigation-title href=https://emavgl.github.io/>Emanuele Viglianisi</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://emavgl.github.io/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://emavgl.github.io/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://emavgl.github.io/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=https://emavgl.github.io/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://emavgl.github.io/posts/third-blog/>Comparing a transformer-based detector with a classical detector</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2022-08-02T00:00:00Z>August 2, 2022</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
2-minute read</span></div><div class=authors><i class="fa fa-user" aria-hidden=true></i>
<a href=https://emavgl.github.io/authors/fabio-geraci/>Fabio Geraci</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://emavgl.github.io/tags/computer-vision/>Computer Vision</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/prostatex/>ProstateX</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/detr/>DETR</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/icevision/>IceVision</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/weight-bias/>Weight & Bias</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/transformer/>transformer</a></span>
<span class=separator>â€¢</span>
<span class=tag><a href=https://emavgl.github.io/tags/object-detection/>Object Detection</a></span></div></div></header><div class=post-content><div><h2>Table Of Contents</h2><nav id=TableOfContents><ul><li><a href=#-motivation>ğŸ”¥ Motivation</a></li><li><a href=#-dataset-description>ğŸ”« Dataset Description</a></li><li><a href=#-set-up>ğŸ”© Set-Up</a></li><li><a href=#results>ğŸ©ºResults</a><ul><li><a href=#-coco-metric-cnn>ğŸ‘ Coco Metric CNN</a></li><li><a href=#-coco-metric-transformers>ğŸ‘ Coco Metric Transformers</a></li></ul></li><li><a href=#-comments--feedback>ğŸ™ Comments & Feedback</a></li><li><a href=#follow-me->Follow me ğŸ‘‡</a></li></ul></nav></div><h2 id=-motivation>ğŸ”¥ Motivation
<a class=heading-link href=#-motivation><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Since the introduction, by <a href=https://paperswithcode.com/method/detr class=external-link target=_blank rel=noopener>Facebook</a>, Transformer based detectors are getting
a lot of traction in <a href=https://en.wikipedia.org/wiki/Computer_vision class=external-link target=_blank rel=noopener>Computer Vision</a> and <a href=https://en.wikipedia.org/wiki/Object_detection class=external-link target=_blank rel=noopener>Object Detection</a>.</p><p>In this blog, I will show how a partially trained detector using SWIN transformer backbone leads to a very competitive performance compared to a fully trained classical detector.</p><h2 id=-dataset-description>ğŸ”« Dataset Description
<a class=heading-link href=#-dataset-description><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><a href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=23691656" class=external-link target=_blank rel=noopener>ProstateX</a> dataset is a collection carefully-curated prostate MRI exams to validate modern AI algorithms and estimate radiologists&rsquo; performance at clinically significant prostate cancer detection and diagnosis.</p><h2 id=-set-up>ğŸ”© Set-Up
<a class=heading-link href=#-set-up><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>For this exercise I will be using:</p><ul><li><a href=https://airctic.com/0.12.0/ class=external-link target=_blank rel=noopener>IceVision</a> which offers a curated collection with hundreds of high-quality pre-trained models from <a href=https://pytorch.org/vision/stable/index.html class=external-link target=_blank rel=noopener>Torchvision</a> and <a href=https://mmdetection.readthedocs.io/en/latest/ class=external-link target=_blank rel=noopener>MMLabs</a>.</li><li><a href=https://wandb.ai/ class=external-link target=_blank rel=noopener>Weight & Bias</a> for tracking model training and metric.</li><li>Train Set 500 images</li><li>Validation Set 100 images</li><li>Applied Augmentation Resize, Flip and Rotation</li></ul><p>Also, I will run experiments on the following architectures:</p><ul><li><p>CNN</p><ul><li>yolof_r50_c5_8x8_1x_coco</li><li>retinanet_r50_fpn_1x_coco</li><li>vfnet_r50_fpn_1x_coco</li><li>tf_efficientdet_lite0</li></ul></li><li><p>Transformers</p><ul><li>vfnet_swin-t-p4-w7_fpn_1x_coco</li><li>retinanet_swin-s-p4-w7_fpn_1x_coco</li><li>detr_r50_8x2_150e_coco</li></ul></li></ul><h2 id=results>ğŸ©ºResults
<a class=heading-link href=#results><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=-coco-metric-cnn>ğŸ‘ Coco Metric CNN
<a class=heading-link href=#-coco-metric-cnn><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>With the described setup vfnet achieves the best score of 0.7747, closely followed by retinanet.<figure><img src=https://emavgl.github.io/posts/assets/pic1_third_post.jpg width=600 height=400></figure></p><h3 id=-coco-metric-transformers>ğŸ‘ Coco Metric Transformers
<a class=heading-link href=#-coco-metric-transformers><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Again vfnet, with Swin Transformer, achieves the best score of 0.7066. To be noticed that only the backbone is trained, so this is an encouraging result.<figure><img src=https://emavgl.github.io/posts/assets/pic2_third_post.png width=600 height=400></figure></p><h2 id=-comments--feedback>ğŸ™ Comments & Feedback
<a class=heading-link href=#-comments--feedback><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>I hope youâ€™ve learned a thing or two from this blog post. If you have any questions, comments, or feedback, please leave them on the following LinkedIn or Twitter.</p><h2 id=follow-me->Follow me ğŸ‘‡
<a class=heading-link href=#follow-me-><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><a href=https://www.linkedin.com/in/fabiogeraci/ target=_blank rel=noreferrer class=markdown-btn>LinkedIn</a>
<a href=https://twitter.com/FGeraci73/ target=_blank rel=noreferrer class=markdown-btn>Twitter</a></p></div><footer></footer></article></section></div><footer class=footer><section class=container>Â©
2020 -
2023
Emanuele Viglianisi
Â·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=https://emavgl.github.io/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>